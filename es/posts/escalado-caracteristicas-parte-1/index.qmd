---
title: "Escalado de Características Parte 1: Cuándo (y Por Qué) Escalar los Datos en Aprendizaje Automático"
author: "January Tabaka"
date: "2025-08-06"
categories: [Fundamentos, Preprocesamiento de Datos, Escalado]
image: "/images/dist_data_landscape.png"
draft: false
lang: es
---

## El Sesgo Invisible en los Datos

Cuando cargas un conjunto de datos para un problema del mundo real, notarás inmediatamente que las características vienen en todo tipo de formas y tamaños.

* Para un modelo de **detección de fraude financiero**, podrías tener:
    - el **importe de la transacción** de un usuario (por ejemplo, 5.00 – 20,000.00),
    - el **número de transacciones realizadas en las últimas 24 horas** (por ejemplo, 1 – 50),
    - el **tiempo desde su último inicio de sesión**, medido en segundos (por ejemplo, 300 – 86,400).

:::{.callout-note collapse="true"}

## Más ejemplos

* En **Analítica de Comercio Electrónico**
    - una columna podría representar **el número de artículos en el carrito de compras del usuario** (por ejemplo, 1-20),
    - otra **el valor total de la compra** en céntimos (por ejemplo, 500-50,000),
    - y una tercera el **índice de valor del cliente a lo largo del tiempo** (por ejemplo, 0-1)

* Para **participación de usuarios en SaaS y Predicción de Cancelación**
    - una característica podría representar el **número de inicios de sesión del usuario este mes** (por ejemplo, 5-100),
    - otra los **minutos totales que pasó en la plataforma** (por ejemplo, 60-10,000),
    - y una tercera la **probabilidad de cancelación calculada** (por ejemplo, 0.05-0.95)

* En un escenario industrial de **Mantenimiento Predictivo**
    - podríamos tener la **temperatura** de una máquina en grados Celsius (por ejemplo, 40-90) como una característica,
    - otra sería sus **vibraciones** en Hz (por ejemplo, 10-2000),
    - y una tercera sus **horas totales de operación** (por ejemplo, 5,000-50,000)

:::

En este escenario, las características existen en escalas completamente diferentes. Un principiante podría sentirse tentado a introducir estos datos directamente en un modelo de aprendizaje automático, pero esto sería un error crítico. Muchos de los algoritmos más comunes y potentes son sensibles a la escala de las características de entrada. Sin una etapa de preprocesamiento, el modelo podría verse desproporcionadamente influenciado por características de gran magnitud, reduciendo su precisión. Esa etapa es el **escalado de características**.

## Por Qué el Escalado es Tan Importante: Una Analogía Visual

Para entender por qué importa el escalado, veamos algunos datos.

Para ilustrarlo, utilizaremos un conjunto de datos del mundo real sobre el mercado inmobiliario de Madrid. El conjunto de datos completo fue originalmente publicado en Kaggle por el usuario [SMADLER92](https://www.kaggle.com/smadler92) y se puede encontrar [aquí](https://www.kaggle.com/code/smadler92/madrid-housing-market-machine-learning-model/input).

Para mantener nuestra visualización clara y reproducible, hemos extraído una muestra aleatoria de 100 propiedades del conjunto de datos original, utilizando un `random_state` de 42.

Es una manera potente de visualizar los dos pasos clave del proceso de estandarización.

Esto es lo que muestra cada etapa de la visualización:

* **1. Datos Originales (Izquierda):** Estos son los datos originales de nuestra muestra. El gráfico está dramáticamente distorsionado, apareciendo como una delgada línea horizontal. Debido a que la característica `Precio (€)` tiene un rango y una varianza inmensamente mayores que la `Superficie (m²)`, la verdadera relación lineal entre las características queda completamente oculta.

* **2. Tras Centrado (Centro):** Este gráfico muestra los datos después de restar la media. Aunque la nube de datos se ha desplazado para centrarse alrededor del origen (0,0), el problema fundamental de la distorsión persiste. El gráfico sigue mostrando una línea horizontal, y la característica `Precio (€)` continúa dominando el eje x.

* **3. Tras Escalado (Derecha):** Esta es la transformación final. Después de dividir por la desviación estándar, la varianza de ambas características se iguala. La distorsión ha desaparecido, y la nube de datos ahora revela una **clara tendencia diagonal**, descubriendo la correlación entre el precio y la superficie.

```{python}
#| label: py-scaling
#| echo: false
#| eval: true
#| fig-cap: "La distribución de las características de viviendas en tres etapas: originales, centrados y completamente escalados. Observa cómo el centrado desplaza los datos, pero solo el paso final de escalado corrige la distorsión."

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Cargar el conjunto de datos
X_train = pd.read_csv("../../data/houses_Madrid_sampled.csv")

mu     = np.mean(X_train,axis=0)   
sigma  = np.std(X_train,axis=0) 
X_mean = (X_train - mu)
X_norm = (X_train - mu)/sigma   

# Define los nombres de las características para las etiquetas
feature_x = 'buy_price'
feature_y = 'sq_mt_built'
X_features = ["Precio (€)", "Superficie (m²)"]

fig, ax = plt.subplots(1, 3, figsize=(15, 4.5))

# Gráfico 1: Datos Originales
ax[0].scatter(X_train[feature_x], X_train[feature_y])
ax[0].set_xlabel(X_features[0])
ax[0].set_ylabel(X_features[1])
ax[0].set_title("1. Datos CrudOriginalesos")
ax[0].axis('equal')
ax[0].grid(True)

# Gráfico 2: Datos Centrados
ax[1].scatter(X_mean[feature_x], X_mean[feature_y])
ax[1].set_xlabel(X_features[0])
ax[1].set_ylabel(X_features[1])
ax[1].set_title(r"2. Tras Centrado (X - $\mu$)")
ax[1].axis('equal')
ax[1].grid(True)

# Gráfico 3: Datos Escalados
ax[2].scatter(X_norm[feature_x], X_norm[feature_y])
ax[2].set_xlabel(X_features[0])
ax[2].set_ylabel(X_features[1])
ax[2].set_title(r"3. Tras Escalado ((X - $\mu$) / $\sigma$)")
ax[2].axis('equal')
ax[2].grid(True)

fig.suptitle("El Efecto Paso a Paso de la Estandarización", fontsize=16)
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()
```

Los modelos que dependen del cálculo de distancias (como **K-NN (K-vecinos más cercanos)**) o que utilizan descenso del gradiente para la optimización (como **Regresión Lineal** y **Redes Neuronales**) son los más afectados por la escala de sus características.

:::{.callout-tip collapse="true"}

## Código para los Gráficos

A continuación se presenta el código Python que carga los datos y crea las visualizaciones. El proceso tiene dos partes: primero, preparar el conjunto de datos, y segundo, generar los gráficos.

### 1. Preparación de los Datos

El código de los gráficos utiliza un archivo de muestra llamado `houses_Madrid_sampled.csv`. Para crearlo, primero descargué el conjunto de datos completo de Kaggle. Luego, utilicé el siguiente script de Python con `pandas` para  extraer una muestra aleatoria de 100 filas y guardar esa muestra en un nuevo archivo CSV.

Este archivo de muestra es el que se incluye en el [repositorio de GitHub del blog](https://github.com/januarytabaka/januarytabaka.github.io) para que el ejemplo sea completamente reproducible.

```{python}
#| label: py-data-prep-code
#| echo: true
#| eval: false

import pandas as pd

# Cargar el conjunto de datos completo (descargado de Kaggle)
full_df = pd.read_csv("houses_Madrid.csv")

# Seleccionar solo las columnas necesarias y eliminar filas con valores nulos
clean_df = full_df[['sq_mt_built', 'buy_price']].dropna()

# Tomar una muestra aleatoria reproducible de 100 propiedades
sampled_df = clean_df.sample(n=100, random_state=42)

# Guardar la muestra en un nuevo archivo CSV
sampled_df.to_csv('houses_Madrid_sampled.csv', index=False)
```

### 2. Código de Visualización

Una vez que tenemos nuestro archivo de muestra, el siguiente código (que es el que se ejecuta para generar los gráficos en esta página) carga houses_Madrid_sampled.csv y crea la visualización de tres pasos.

Está inspirado en el laboratorio "Feature Scaling and Learning Rate (Multi-variable)" del curso Supervised Machine Learning: Regression and Classification de la [deeplearning.ai](https://www.deeplearning.ai/), impartido por el profesor Andrew Ng.

```{python}
#| label: py-scaling-code
#| echo: true
#| eval: false
#| fig-cap: "La distribución de las características de viviendas en tres etapas: originales, centrados y completamente escalados. Observa cómo el centrado desplaza los datos, pero solo el paso final de escalado corrige la distorsión."

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Cargar el conjunto de datos
X_train = pd.read_csv("../../data/houses_Madrid_sampled.csv")

mu     = np.mean(X_train,axis=0)   
sigma  = np.std(X_train,axis=0) 
X_mean = (X_train - mu)
X_norm = (X_train - mu)/sigma   

# Define los nombres de las características para las etiquetas
feature_x = 'buy_price'
feature_y = 'sq_mt_built'
X_features = ["Precio (€)", "Superficie (m²)"]

fig, ax = plt.subplots(1, 3, figsize=(15, 4.5))

# Gráfico 1: Datos Originales
ax[0].scatter(X_train[feature_x], X_train[feature_y])
ax[0].set_xlabel(X_features[0])
ax[0].set_ylabel(X_features[1])
ax[0].set_title("1. Datos CrudOriginalesos")
ax[0].axis('equal')
ax[0].grid(True)

# Gráfico 1: Datos Centrados
ax[1].scatter(X_mean[feature_x], X_mean[feature_y])
ax[1].set_xlabel(X_features[0])
ax[1].set_ylabel(X_features[1])
ax[1].set_title(r"2. Tras Centrado (X - $\mu$)")
ax[1].axis('equal')
ax[1].grid(True)

# Gráfico 1: Datos Escalados
ax[2].scatter(X_norm[feature_x], X_norm[feature_y])
ax[2].set_xlabel(X_features[0])
ax[2].set_ylabel(X_features[1])
ax[2].set_title(r"3. Tras Escalado ((X - $\mu$) / $\sigma$)")
ax[2].axis('equal')
ax[2].grid(True)

fig.suptitle("El Efecto Paso a Paso de la Estandarización", fontsize=16)
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()
```

:::

:::{.callout-important collapse="true"}
## Una Mirada Más Profunda: Optimización y Regularización

Más allá de la intuición visual, hay dos razones técnicas clave por las que el escalado es tan crítico para ciertos modelos.

#### Regresión Lineal: Optimización vs. Interpretabilidad

La **Regresión Lineal** se beneficia del escalado, pero el "por qué" depende de cómo se entrene el modelo.

* Con **descenso del gradiente**, el escalado es esencial. Las características sin escalar generan una superficie de pérdida sesgada y elíptica, lo que hace que el optimizador avance lentamente en zigzag hacia el mínimo. El escalado transforma la superficie en una forma más uniforme, permitiendo una convergencia más rápida y estable.

* Con la **ecuación normal**, el escalado no es necesario para la corrección matemática, pero sigue siendo altamente recomendable por **interpretabilidad**. Sin escalado, las magnitudes de los coeficientes reflejan las unidades de las características y no son directamente comparables. Estandarizar permite comparar los tamaños de los coeficientes de forma significativa.

#### Regularización: Garantizar una Penalización Justa

El escalado se vuelve **obligatorio** cuando se utiliza **regularización** (como L1/Lasso o L2/Ridge). Estas técnicas penalizan los coeficientes grandes para evitar el sobreajuste. Sin embargo, la penalización se aplica de forma uniforme, sin tener en cuenta la escala de las características.
Si las características varían en escala, aquellas con rangos numéricos mayores serán penalizadas más severamente, sin importar su importancia real. Esto puede llevar a una reducción incorrecta de características informativas.

Al escalar los datos, nos aseguramos de que la regularización trate a todas las características por igual, permitiendo que el modelo evalúe con mayor precisión la verdadera relevancia de cada variable.

:::

## Entonces, ¿Cuándo Se Debería Escalar los Datos?

Aquí llegamos al quid de la cuestión. Aunque el escalado es crítico para la mayoría de los algoritmos, **no** es un requisito universal.

Algunas excepciones son los **modelos basados en árboles**. Algoritmos como los Árboles de Decisión, Random Forests y modelos de Gradient Boosting (como XGBoost) generalmente no son sensibles a la escala de las características.

Esto se debe a que estos modelos funcionan realizando una serie de divisiones sobre características individuales. Un árbol de decisión formula preguntas como: "¿Es el ingreso mayor que €100,000?" o "¿Es la edad menor que 40?". La respuesta a estas preguntas es la misma, ya sea que el ingreso esté en euros o en miles de euros. La escala de otras características no afecta la división que se elige para una característica particular.

Para casi todos los demás casos, sin embargo, el escalado debe ser un paso fundamental en el pipeline de modelado.

## ¿Qué Sigue?

Ahora que entendemos cuándo y por qué el escalado es un paso fundamental, es momento de adentrarnos en el cómo. En la próxima entrada, exploraremos tres de las técnicas de escalado más utilizadas—StandardScaler, MinMaxScaler y RobustScaler—en scikit-learn de Python, junto con sus equivalentes en R. Cada método se demostrará con código práctico y ejecutable para ayudar a aplicarlos con confianza en los proyectos.